{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformation: Use spark to generate joined word embeddings\n",
    "\n",
    "In this notebook, we pick up the output of the last Airflow operator in `download_and_ocr_images` which was writing partitioned table to Google BigQuery for individual save_key words we queried Bing for. We combine all the _partioned tables and compute word embeddings for them, that can be later used for assigning features to the graph nodes (currently out of scope). As a result of this, we can generate dashboards based on word counts per save_key etc. \n",
    "\n",
    "The Notebook using `pyspark` covers the following steps:\n",
    "\n",
    "* Check all tables in bigquery datasets with _partitioned\n",
    "* Load them and union them to a single spark DataFrame taking into account the seach_key (named documentType)\n",
    "* Preprocess tokens: lowercase, remove punctuation\n",
    "* Compute word embeddings using spacy\n",
    "* Write joined `embedding` table back to Bigquery dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import SparkSession\n",
    "from udfs import (\n",
    "    spacy_word2vec_grouped_udf,\n",
    "    remove_punctuation_udf,\n",
    "    lowercase_udf\n",
    ")\n",
    "\n",
    "# Load environment variables from .env file for more flexibility\n",
    "dotenv.load_dotenv(dotenv.find_dotenv(\".env\"), override=True)\n",
    "\n",
    "home = os.path.expanduser(\"~\")\n",
    "project_id = os.environ[\"GCP_PROJECT_ID\"]\n",
    "bigquery_dataset = os.environ[\"BIGQUERY_DATASET\"]\n",
    "bucket = os.environ[\"GCP_GCS_BUCKET\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The most exhaustive task is to compute word embeddings for the OCRed tokens. To split up the work create partitions for that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_partitions_word2vec = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize spark session. Please note the `README.md` file denoting download of additional jars. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"test\") \\\n",
    "    .config(\"spark.jars\", f\"{home}/bin/spark-3.0.3-bin-hadoop3.2/jars/spark-bigquery-latest_2.12.jar\") \\\n",
    "    .config(\"spark.jars\", f\"{home}/bin/spark-3.0.3-bin-hadoop3.2/jars/gcs-connector-hadoop2-2.1.1.jar\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all tables in the dataset, which contain _partitioned. These are the ones, created in the last step of airflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"materializationDataset\", bigquery_dataset)\n",
    "\n",
    "tables = (\n",
    "    spark.read.format(\"bigquery\")\n",
    "    .option(\"project\", project_id)\n",
    "    .option(\"viewsEnabled\", \"true\")\n",
    "    .load(f\"select table_name from {project_id}.{bigquery_dataset}.INFORMATION_SCHEMA.TABLES\")\n",
    ").toPandas()[\"table_name\"]\n",
    "\n",
    "tables = tables[tables.str.endswith(\"_partitioned\")].tolist()\n",
    "tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "Load the tables and union them into a single table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "queries = []\n",
    "\n",
    "for table in tables:\n",
    "    \n",
    "    documentType = table.split(\"_partitioned\")[0]\n",
    "\n",
    "    tab = (\n",
    "        spark.read.format(\"bigquery\")\n",
    "        .option(\"project\", project_id)\n",
    "        .option(\"table\", f\"{bigquery_dataset}.{table}\")\n",
    "        .load()\n",
    "    )\n",
    "    tab.registerTempTable(table)\n",
    "    queries.append(\n",
    "        f\"\"\"SELECT *, '{documentType}' AS documentType FROM {table}\"\"\"\n",
    "    )\n",
    "    \n",
    "query = \" UNION ALL \".join(queries)\n",
    "\n",
    "ocr = spark.sql(query)\n",
    "ocr.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the tokens to be vectorized by\n",
    "\n",
    "* Removing punctuation\n",
    "* make lowercase\n",
    "* more to come ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ocr = (\n",
    "    ocr\n",
    "    .withColumn(\"text\", \n",
    "        remove_punctuation_udf(F.col(\"text\"))\n",
    "    )\n",
    "    .withColumn(\"text\", \n",
    "        lowercase_udf(F.col(\"text\"))\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, reorganize the data and compute word embeddings using spacy. Seems kind of hacky, however, is the only working method found. Check this out https://towardsdatascience.com/a-couple-tricks-for-using-spacy-at-scale-54affd8326cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding = (\n",
    "    ocr\n",
    "    .select('id', 'documentId', 'documentType', 'text')\n",
    "    .groupby((F.floor(F.rand() * n_partitions_word2vec)).alias('groupNumber'))\n",
    "    .agg(F.collect_list(F.struct(F.col('id'), F.col(\"documentId\"), F.col(\"documentType\"), F.col('text'))).alias('documentGroup'))\n",
    "    .repartition('groupNumber')\n",
    "    .select(F.explode(spacy_word2vec_grouped_udf(F.col('documentGroup'))).alias('results'))\n",
    "    .select(F.col('results.*'))\n",
    "    .select(\"id\", \"documentId\", \"documentType\", \"text\", \"vector\")\n",
    "    .sort(F.col(\"documentId\"), F.col(\"id\"))\n",
    "    .join(\n",
    "        ocr.select('id', 'documentId', 'block_num', 'line_num', 'left', 'top', 'width', 'height', 'conf'),\n",
    "        on=['id', 'documentId'], \n",
    "        )\n",
    "    .withColumnRenamed(\"id\", \"textId\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "embedding.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may take a while. Really heavy part is actually writing to BigQuery for some reason. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding.write.format('bigquery') \\\n",
    "   .option(\"project\", project_id) \\\n",
    "   .option(\"temporaryGcsBucket\", bucket) \\\n",
    "   .mode(\"overwrite\") \\\n",
    "   .save(f\"{bigquery_dataset}.embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e2d84faae671fe89ef1a9f8bfbf3ae3364610589377d1f9ea3abec866f932907"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
